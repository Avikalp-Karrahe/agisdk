REAL Bench is a benchmark for evaluating web AI agents, developed by AGI, Inc. <mcreference link="https://www.theagi.company/blog/introducing-real-bench" index="1">1</mcreference> It provides a standardized testing environment with high-fidelity replicas of popular websites like Amazon, DoorDash, and Airbnb. <mcreference link="https://www.theagi.company/blog/introducing-real-bench" index="1">1</mcreference> <mcreference link="https://github.com/agi-inc/agisdk" index="2">2</mcreference>

The primary goal of REAL Bench is to address the shortcomings of existing AI agent evaluation frameworks, which are often too simplistic and fail to capture the complexities of real-world web interactions. <mcreference link="https://www.theagi.company/blog/introducing-real-bench" index="1">1</mcreference> REAL Bench aims to provide a realistic and comprehensive testing ground for AI agents, allowing developers and researchers to:
- Assess real-world performance on web environments that mimic actual websites. <mcreference link="https://www.theagi.company/blog/introducing-real-bench" index="1">1</mcreference>
- Ensure security and reliability by testing agents in scenarios requiring adherence to security protocols. <mcreference link="https://www.theagi.company/blog/introducing-real-bench" index="1">1</mcreference>
- Benchmark agent performance against industry standards using standardized metrics. <mcreference link="https://www.theagi.company/blog/introducing-real-bench" index="1">1</mcreference>

REAL Bench features a public leaderboard where developers can submit their agents' performance, fostering a competitive and collaborative environment to encourage continuous improvement and innovation. <mcreference link="https://www.theagi.company/blog/introducing-real-bench" index="1">1</mcreference> The AGI SDK is used in conjunction with REAL Bench to build and evaluate these AI browser agents. <mcreference link="https://github.com/agi-inc/agisdk" index="2">2</mcreference>

Initial benchmark results on REAL Bench show that reasoning models like Claude-3.7-Sonnet-Thinking perform better than standard pre-trained models, but even frontier models face challenges such as inaccurate task completion verification, getting stuck in navigation, and handling complex UI elements or multi-step tasks. <mcreference link="https://www.theagi.company/blog/introducing-real-bench" index="1">1</mcreference> Open-source models currently lag significantly behind closed-source models in these evaluations. <mcreference link="https://www.theagi.company/blog/introducing-real-bench" index="1">1</mcreference>