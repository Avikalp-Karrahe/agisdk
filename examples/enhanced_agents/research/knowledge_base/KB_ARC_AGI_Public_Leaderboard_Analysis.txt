The ARC-AGI Public Leaderboard (ARC-AGI-Pub) has been introduced to evaluate AI models against the ARC-AGI benchmark using a public evaluation dataset. <mcreference link="https://arcprize.org/blog/introducing-arc-agi-public-leaderboard" index="2">2</mcreference> Unlike the official ARC Prize competition on Kaggle, this public leaderboard lifts compute restrictions and allows internet access. <mcreference link="https://arcprize.org/blog/introducing-arc-agi-public-leaderboard" index="2">2</mcreference>

The ARC-AGI benchmark has evolved, with ARC-AGI-2 challenging systems on both high adaptability and high efficiency. <mcreference link="https://arcprize.org/leaderboard" index="1">1</mcreference> The leaderboard displays the relationship between cost-per-task and performance, emphasizing intelligence efficiency. <mcreference link="https://arcprize.org/leaderboard" index="1">1</mcreference>

There are two main leaderboards now: <mcreference link="https://arcprize.org/blog/introducing-arc-agi-public-leaderboard" index="2">2</mcreference>
1.  **ARC-AGI Leaderboard (Kaggle):** Uses a private evaluation dataset, has no internet access, and enforces compute limits. This is for the official ARC Prize competition. <mcreference link="https://arcprize.org/blog/introducing-arc-agi-public-leaderboard" index="2">2</mcreference>
2.  **ARC-AGI-Pub Leaderboard (arcprize.org):** Uses a public evaluation dataset, allows internet access, and has no enforced compute limits (though there's an enforced cost limit). High scores are verified and published with reproducible open-source code. <mcreference link="https://arcprize.org/blog/introducing-arc-agi-public-leaderboard" index="2">2</mcreference>

A $150,000 USD verification fund has been committed to support the new public leaderboard, reimbursing costs for verified high-score claims. <mcreference link="https://arcprize.org/blog/introducing-arc-agi-public-leaderboard" index="2">2</mcreference>

Initial results on the ARC-AGI-Pub leaderboard include scores from novel approaches (e.g., Ryan Greenblatt using GPT-4o to generate and refine Python programs), baselines for leading LLMs (OpenAI GPT-4o, Anthropic Claude 3.5 Sonnet, Google Gemini 1.5), and past competition-winning approaches. <mcreference link="https://arcprize.org/blog/introducing-arc-agi-public-leaderboard" index="2">2</mcreference> Human performance data is also included as a benchmark. <mcreference link="https://arcprize.org/leaderboard" index="1">1</mcreference>